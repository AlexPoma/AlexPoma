{
    "es" : {
        "CV_WE_work_exp" : "Experiencia de Trabajo",

        "CV_WE_01_job_name" : "Ingeniero de Datos",
        "CV_WE_01_emp_date" : "TCS - RIMAC | Nov 2020 - Act.",
        "CV_WE_01_desc" : "Soy parte de la Tribu Data, miembro del equipo de AWS Support, responsable de crear y mantener diferentes pipelines de datos usando tecnologías Cloud y metodologías SCRUM.",
        "CV_WE_01_li_01" : "Parte del proyecto de RIMAC Seguros para la implementación del ecosistemas de BigData en AWS.",
        "CV_WE_01_li_02" : "Creación de flujos de trabajo usando AWS Step Function.",
        "CV_WE_01_li_03" : "Creación de Jobs con Python en AWS Glue para la automatización de procesamiento de datos.",
        "CV_WE_01_li_04" : "Generación de triggers en AWS Lambda.",
        "CV_WE_01_li_05" : "Programación de horarios con AWS CloudWatch.",
        "CV_WE_01_li_06" : "Creación de tablas y scripts en Athena y Redshift.",
        "CV_WE_01_li_07" : "Implementación y optimización de scripts en Presto SQL.",
        "CV_WE_01_li_08" : "Revisión de logs, corrección y optimización de potenciales errores.",
        "CV_WE_01_li_09" : "Creación y optimización de Dashboards usando Power BI.",
        "CV_WE_01_li_10" : "Uso de metodologías Agiles complementadas con Scrum.",
        "CV_WE_Skills_01" : "AWS | Lambda | Glue | Step Function | CloudWatch | Athena | Redshift | Python | Presto SQL | PowerBI | Power Automate",

        "CV_WE_02_job_name" : "Analista de Inteligencia de Negocios",
        "CV_WE_02_emp_date" : "CMAC Tacna | Jun 2018 - Nov 2020",
        "CV_WE_02_desc" : "Dentro de la Caja Tacna tuve la oportunidad de aprender muchas cosas sobre el sistema financiero, además de ser parte del área de Inteligencia de negocios y Soluciones Analíticas, donde pude generar y optimizar Scripts en SQLServer, usar herramientas ETL para la creación de cubos OLAP y la generación de Dashboards de Power BI.",
        "CV_WE_02_li_01" : "Implementación y administración del Data Warehouse.",
        "CV_WE_02_li_02" : "Generación de reportes crediticios usando SSRS.",
        "CV_WE_02_li_03" : "Generación de paquetes de carga de datos usando SSIS.",
        "CV_WE_02_li_04" : "Implementación de Cubos OLAP usando SSAS, para la generación de reportes gerenciales.",
        "CV_WE_02_li_05" : "Automatización del proceso de extracción de datos de la SBS usando SSIS y R.",
        "CV_WE_02_li_06" : "Reducción de tiempos para la generación de cosechas, con la implementación de un Cubo OLAP.",
        "CV_WE_02_li_07" : "Implementación de Dashboards usando Power BI.",
        "CV_WE_02_li_08" : "Generación y mejora de Scripts en SQL Server.",
        "CV_WE_02_li_09" : "Implementación del proyecto Sinergia que abarca los reportes y tableros gerenciales mas utilizados y de mayor importancia para la toma de decisiones de la empresa.",
        "CV_WE_Skills_02" : "SSIS | SSRS | SSAS | PowerBI | MSQL Server | R | DAX | MDX",

        "CV_WE_03_job_name" : "Asistente Técnico - Desarrollador Machine Learning",
        "CV_WE_03_emp_date" : "UNJBG - H2O | Sep 2020 - Jul 2021",
        "CV_WE_03_desc" : "Apoye al grupo de Investigación del agua de la Universidad Nacional Jorge Basadre Grohmann, generando modelos estadísticos y de machine learning, usando R y Python.",
        "CV_WE_03_li_01" : "Se crearon diferentes tipos de gráficos estadísticos usando R y Python, siguiendo las especificaciones de los investigadores.",
        "CV_WE_03_li_02" : "Automatice diferentes procesos manuales para que los investigadores se centren en el análisis de los datos.",
        "CV_WE_03_li_03" : "Se generaron interfaces en Shiny con gráficos interactivos.",
        "CV_WE_03_li_04" : "He creado Notebooks en Jupyter y R Markdown.",
        "CV_WE_03_li_05" : "Se desarrollo un modelo de Redes Neuronales usando TensorFlow para la predicción de la Evapotranspiración.",
        "CV_WE_Skills_03" : "Python | R | Shiny | Jupyter | Neural Network | Machine Learning"
    },
    "en" : {
        "CV_WE_work_exp" : "Work Experiences",

        "CV_WE_01_job_name" : "Data Engineer",
        "CV_WE_01_emp_date" : "TCS - RIMAC | Nov 2020 - Now",
        "CV_WE_01_desc" : "I'm part of the Tribu Data, a member of the AWS Support team, responsible for creating and maintaining different data pipelines using Cloud technologies and SCRUM methodologies.",
        "CV_WE_01_li_01" : "Part of the RIMAC Seguros project for the implementation of the BigData ecosystems on AWS.",
        "CV_WE_01_li_02" : "Create workflows using AWS Step Function.",
        "CV_WE_01_li_03" : "Creation of Python Jobs on AWS Glue for data processing automation.",
        "CV_WE_01_li_04" : "Triggers generation in AWS Lambda.",
        "CV_WE_01_li_05" : "Scheduling works with AWS CloudWatch.",
        "CV_WE_01_li_06" : "Creation of tables and scripts in Athena and Redshift.",
        "CV_WE_01_li_07" : "Implementation and optimization of scripts in Presto SQL.",
        "CV_WE_01_li_08" : "Logs review, correction and optimization of potential errors.",
        "CV_WE_01_li_09" : "Creation and optimization of Dashboards using Power BI.",
        "CV_WE_01_li_10" : "Use of Agile methodologies complemented with Scrum.",
        "CV_WE_Skills_01" : "AWS | Lambda | Glue | Step Function | CloudWatch | Athena | Redshift | Python | Presto SQL | PowerBI | Power Automate",

        "CV_WE_02_job_name" : "Business Intelligence Analyst",
        "CV_WE_02_emp_date" : "CMAC Tacna | Jun 2018 - Nov 2020",
        "CV_WE_02_desc" : "Inside the Caja Tacna I had the opportunity to learn many things about the financial system, also to being part of the Business Intelligence and Analytical Solutions area, where I was able to generate and optimize Scripts in SQLServer, use ETL tools to create OLAP cubes and the generation of Power BI Dashboards.",
        "CV_WE_02_li_01" : "Implementation and administration of the Data Warehouse.",
        "CV_WE_02_li_02" : "Generation of credit reports using SSRS.",
        "CV_WE_02_li_03" : "Generation of data load packages using SSIS.",
        "CV_WE_02_li_04" : "Implementation of OLAP Cubes using SSAS, for the generation of managerial reports.",
        "CV_WE_02_li_05" : "Automating the SBS Data Extraction Process using SSIS and R.",
        "CV_WE_02_li_06" : "Times reduction for the generation of financial periods, with the implementation of an OLAP Cube.",
        "CV_WE_02_li_07" : "Implementation of Dashboards using Power BI.",
        "CV_WE_02_li_08" : "Generation and improvement of Scripts in SQL Server.",
        "CV_WE_02_li_09" : "Implementation of the Sinergia project that covers the most used and most important reports and management boards for the company's decision making.",
        "CV_WE_Skills_02" : "SSIS | SSRS | SSAS | PowerBI | MSQL Server | R | DAX | MDX",

        "CV_WE_03_job_name" : "Technical Assistance - ML Developer",
        "CV_WE_03_emp_date" : "UNJBG - H2O | Sep 2020 - Jul 2021",
        "CV_WE_03_desc" : "Support the Water Research group of the Jorge Basadre Grohmann National University, generating statistical and machine learning models, using R and Python.",
        "CV_WE_03_li_01" : "Multiple types of statistical graphs were created using R and Python, using the researchers specifications.",
        "CV_WE_03_li_02" : "Automate multiple manual processes so allow researchers focus on data analysis.",
        "CV_WE_03_li_03" : "Shiny UI with interactive graphics were generated.",
        "CV_WE_03_li_04" : "I have created Notebooks in Jupyter and R Markdown.",
        "CV_WE_03_li_05" : "A Neural Networks model was developed using TensorFlow for Evapotranspiration prediction.",
        "CV_WE_Skills_03" : "Python | R | Shiny | Jupyter | Neural Network | Machine Learning"
    }
}